{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFimsaoY-Sz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(7)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjuc_OBC_Gso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content = plt.imread('download.jpg')\n",
        "style = plt.imread('abstract.jpg')\n",
        "\n",
        "# Display the images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Content and style images side-by-side\n",
        "ax1.imshow(content)\n",
        "ax1.set_title('Content Image')\n",
        "ax2.imshow(style)\n",
        "ax2.set_title('Style Image')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDgGAxMe_RW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(image):\n",
        "  image = plt.imread(image)\n",
        "  img = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  img = tf.image.resize(img, [160, 250])\n",
        "  # Shape -> (batch_size, h, w, d)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEuyjEdl_ccA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content = load_image('download.jpg')\n",
        "style = load_image('abs2.jpg')\n",
        "\n",
        "# Verify the shapes\n",
        "content.shape, style.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK96t_xN_kg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTsgr8dE_n_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fPDm9LD_q80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_layers = ['block4_conv2']\n",
        "\n",
        "# Style layer\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiuzwSqY_uuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mini_model(layer_names, model):\n",
        "\n",
        "  outputs = [model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = Model([vgg.input], outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OaI7oWT_xht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "  temp = tensor\n",
        "  temp = tf.squeeze(temp)\n",
        "  fun = tf.reshape(temp, [temp.shape[2], temp.shape[0]*temp.shape[1]])\n",
        "  result = tf.matmul(temp, temp, transpose_b=True)\n",
        "  gram = tf.expand_dims(result, axis=0)\n",
        "  return gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpH6hOz_0Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Custom_Style_Model(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(Custom_Style_Model, self).__init__()\n",
        "    self.vgg =  mini_model(style_layers + content_layers, vgg)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Scale back the pixel values\n",
        "    inputs = inputs*255.0\n",
        "    # Preprocess them with respect to VGG19 stats\n",
        "    preprocessed_input = preprocess_input(inputs)\n",
        "    # Pass through the mini network\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    # Segregate the style and content representations\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                      outputs[self.num_style_layers:])\n",
        "\n",
        "    # Calculate the gram matrix for each layer\n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                     for style_output in style_outputs]\n",
        "\n",
        "    # Assign the content representation and gram matrix in\n",
        "    # a layer by layer fashion in dicts\n",
        "    content_dict = {content_name:value\n",
        "                    for content_name, value\n",
        "                    in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "    style_dict = {style_name:value\n",
        "                  for style_name, value\n",
        "                  in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "    return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IiyXj4U_4b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = Custom_Style_Model(style_layers, content_layers)\n",
        "style_targets = extractor(style)['style']\n",
        "content_targets = extractor(content)['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlhzfrA3_7yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=0.002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A0nzTJsAAVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_weight=1000\n",
        "content_weight=100\n",
        "\n",
        "# Custom weights for different style layers\n",
        "style_weights = {'block1_conv1': 1.,\n",
        "                 'block2_conv1': 0.8,\n",
        "                 'block3_conv1': 0.5,\n",
        "                 'block4_conv1': 0.3,\n",
        "                 'block5_conv1': 0.1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwazu3TfAEY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([style_weights[name]*tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n",
        "                           for name in style_outputs.keys()])\n",
        "    # Normalize\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n",
        "                             for name in content_outputs.keys()])\n",
        "    # Normalize\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG1vGj4bAHo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Extract the features\n",
        "    outputs = extractor(image)\n",
        "    # Calculate the loss\n",
        "    loss = total_loss(outputs)\n",
        "  # Determine the gradients of the loss function w.r.t the image pixels\n",
        "  grad = tape.gradient(loss, image)\n",
        "  # Update the pixels\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  # Clip the pixel values that fall outside the range of [0,1]\n",
        "  image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRgEkbJWAKjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_image = tf.Variable(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXJlycNFANg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 6\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(target_image)\n",
        "  plt.imshow(np.squeeze(target_image.read_value(), 0))\n",
        "  plt.title(\"Train step: {}\".format(step))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RddcZFPyIfUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}